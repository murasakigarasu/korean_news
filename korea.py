# -*- coding: utf-8 -*-
"""Korea.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sT2LAcE95yQRe7DPSlaUcQ-OLEb5JVyU
"""

import re
import os
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd

!pip install stanza
import stanza
stanza.download("ko")
nlp_stanza = stanza.Pipeline(lang="ko", processors="tokenize, pos, lemma, depparse")

from sklearn.feature_extraction.text import TfidfVectorizer

path = 'kor/'
corpus = []
for filename in tqdm(os.listdir(path)):
    with open(path + filename, encoding='utf-8') as txt:
        text = txt.read()
        corpus.append(text)

def cleanizer(corpus):
    a1 = ''.join(corpus)
    a1 = re.sub(r'\n', ' ', a1)
    a1 = re.sub(r'[A-Za-z]', '', a1)
    a1 = re.sub(r'\d', '', a1)
    a1 = re.sub(r'\(', '', a1)
    clean_corpus = re.sub(r'\)', '', a1)
    return clean_corpus

clean_corpus_kr = cleanizer(corpus)
clean_corpus_kr[:100]

corp_doc = nlp_stanza(clean_corpus_kr)

def stanza_to_df(corp_doc):
    list_of_rows = []
    counter = 0
    for sentence in corp_doc.sentences:
        counter += 1
        for word in sentence.words:
            list_of_rows.append([counter, word.id, word.text, word.lemma, word.upos, word.deprel, word.head, sentence.words[word.head-1].text])

    df_sentence = pd.DataFrame(list_of_rows, columns=['sent_id', 'id', 'token', 'lemma', 'pos', 'synt_tag', 'head_id', 'head_tok'])
    return df_sentence

df_kr = stanza_to_df(corp_doc)
df_kr[:10]

df_kr_clean = df_kr.loc[(df_kr['synt_tag'] != 'punct')]
df_kr_clean.info()

grouped_tokens = df_kr_clean.groupby('sent_id')['token'].apply(list)
sentences = grouped_tokens.tolist()

df_kr_clean.to_csv('out.csv', index=False)

tokens_kr = df_kr_clean.get('token')
tokens_list_kr = tokens_kr.to_list()
print(len(set(tokens_list_kr)))

def skipgrammer(tokens, window_size):
    skip_grams = []
    for index in range(len(tokens)):
        target_word = tokens[index]
        start = max(0, index - window_size)
        end = min(len(tokens), index + window_size + 1)

        for s_index in range(start, end):
            if index != s_index:
                context = tokens[s_index]
                skip_grams.append((target_word, context))
    return skip_grams

skip_grams_zorg = skipgrammer(tokens_list_kr, 3)

from collections import Counter

target = '지난'
target_skipgrams = []
for skipgram in skip_grams_zorg:
    if skipgram[0] == target or skipgram[1] == target:
        target_skipgrams.append(skipgram)

target_skipgram_counts = Counter(target_skipgrams)
print(target_skipgram_counts.most_common())

!pip install gensim
import gensim

w2v_kr = gensim.models.Word2Vec(sentences, vector_size=300, window=5, min_count=2, sg=0, epochs=5)

freq_tokens_kr = w2v_kr.wv.key_to_index
words_kr = list(freq_tokens_kr.keys())[:200]
print(words_kr)

big_string=''
for i in range(len(tokens_list_kr)):
    big_string+=(tokens_list_kr[i]+' ')

!pip install wordcloud

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS

print('Для корпуса: ', w2v_kr.wv.most_similar('서울', topn=10))

w2v_kr.wv.save_word2vec_format('korea.bin', binary=True)

clean_texts = []
text_no_stop = ' '.join([token for token in tokens_list_kr])
clean_texts.append(text_no_stop)

tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))
tfidf_matrix = tfidf_vectorizer.fit_transform(clean_texts)
feature_names = tfidf_vectorizer.get_feature_names_out()
tfidf = tfidf_matrix.toarray().flatten()
top_indices = tfidf.argsort()[-20:][::-1]
print([(feature_names[i], tfidf[i]) for i in top_indices])

